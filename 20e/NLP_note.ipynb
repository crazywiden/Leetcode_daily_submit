{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week3\n",
    "Convolutional Neural Networks for Text\n",
    "\n",
    "Feature Combinations\n",
    "\n",
    "CNNs and Key Concepts\n",
    "\n",
    "Case Study on Sentiment Classification\n",
    "\n",
    "Structured CNNs\n",
    "\n",
    "Summary\n",
    "\n",
    "\n",
    "Prediciton Problem (Sentiment Classification)\n",
    "\n",
    "Method 1: CBOW\n",
    "1. One of the simplest methods\n",
    "2. Discrete symbols to continuous vectors\n",
    "3. Average all vectors\n",
    "\n",
    "Method 2: Deep CBOW\n",
    "1. More linear transformationns followed by activation functions (Multilayer Perceptron, MLP)\n",
    "Better performance is not guaranteed.\n",
    "\n",
    "What's the Use of the \"Deep\"\n",
    "1. easily to learn feature combinations\n",
    "\n",
    "Method 3: Bag of n-grams\n",
    "- A contiguous sequence of words\n",
    "Concatenate word vectors\n",
    "sum(word1,word2,...,wordn,bias)=scores-softmax->probs\n",
    "wordi = the unicode vector of the i-th word\n",
    "- allow us to capture combination features in a simple way\n",
    "- decent baseline model\n",
    "\n",
    "Problems of Bag of n-grams:\n",
    "1. parameter explosion\n",
    "2. No sharing between similar words/n-grams\n",
    "3. Lose the global sequence order\n",
    "\n",
    "Neural Sequence Models\n",
    "Sequence of words(characters)-->Composition Function(CBOW, Bag of n-grams, CNNs, RNNs, Tranformer, GraphNNs)-->Predict\n",
    "Most of NLP tasks->Sequence representation learning problem\n",
    "\n",
    "Definition of Convolution\n",
    "Convolution --> mathematical operation\n",
    "Continuous:(f*g)(t) = integral of f(t-tau)g(tau)d(tau)\n",
    "Discrete: (f*g)[n] = sum of f[n]g[m] m from -M to M\n",
    "\n",
    "Intuitive Understanding\n",
    "Input: feature vector\n",
    "Filter: learnable parameter\n",
    "Output: hidden vector\n",
    "\n",
    "Priori Entailed by CNNs\n",
    "Local bias: Different words could interact with their neighbours\n",
    "Parameter sharing: The parameters of composition function are the same.\n",
    "\n",
    "Basics of CNNs\n",
    "2d Convolution => matrix\n",
    "Stride: the number of units shifts over the input matrix.\n",
    "Padding: dealing with the units at the boundary of input vector.\n",
    "\n",
    "Three types of Convolutions:\n",
    "Narrow: the length of input is longer than the length of output\n",
    "Equal: the length of input equals to the length of output\n",
    "Wide: the length of input is smaller than the length of output\n",
    "\n",
    "Multiple Filters\n",
    "Each filter represents a unique feature of convolution window.\n",
    "\n",
    "Pooling: an aggregation operation, aiming to select informative features.\n",
    "1. Max pooling: most common, did you see this feature anywhere in the range?\n",
    "2. Average pooling: how prevalent is this feature over the entire range?\n",
    "3. k-Max pooling: did you see this feature up to k times?\n",
    "4. Dynamic pooling; did you see this feature in the beginning? in the middle? in the end?\n",
    "\n",
    "Case Study:\n",
    "Convolutional Networks for Text Classification\n",
    "Task: sentiment classification\n",
    "Input: a sentence\n",
    "Output: a class label(positive/negative)\n",
    "Model:\n",
    "Embedding layer, Multi-Channel CNN layer, Pooling layer/Output layer\n",
    "\n",
    "Embedding layer:\n",
    "Build a look-up table(pre-trained? fine-tuned?)\n",
    "Discrete-->distributed\n",
    "\n",
    "Convolution Layer:\n",
    "Stride size?\n",
    "Wide/ equal/ narrow?\n",
    "How many filters?\n",
    "\n",
    "Pooling Layer\n",
    "Max-pooling\n",
    "Concatenate\n",
    "\n",
    "Output Layer\n",
    "MLP layer\n",
    "Dropout\n",
    "Softmax\n",
    "\n",
    "CNN Variants\n",
    "1. Priori Entailed by CNNs\n",
    "- Local bias: How to handle long-term dependencies?=>increase receptive fields(dilated)\n",
    "- Parameter sharing: How to handle different types of compositionality?(Complicated Interaction)=>dynamic filters\n",
    "\n",
    "Dilated Convolution\n",
    "- Long-term dependency with less layers\n",
    "- Parameters of filters are static, failing to capture rich interaction patterns\n",
    "- Filters are generated dynamically conditioned on an input\n",
    "\n",
    "Common Applications\n",
    "CNN Applications:\n",
    "1. Word-level CNNs\n",
    "- Basic unit: word\n",
    "- Learn the representation of a sentence\n",
    "- Phrasal patterns\n",
    "==>\n",
    "- Sentence representation\n",
    "NLP (Almost) from Scratch\n",
    "\n",
    "2. Char-level CNNs\n",
    "- Basic unit: chatacter\n",
    "- Learn the representation of a word\n",
    "- Extract morphological patterns\n",
    "==>\n",
    "Text Classification\n",
    "CNN-RNN-CRF for Tagging\n",
    "\n",
    "Structured Convolution\n",
    "Why Structured Convolution?\n",
    "- Some convolutional operations are not necessary(E.g.noun-verb pairs very informative, but not captured by normal CNNs)\n",
    "- Language has structure, would like it to localize features.\n",
    "==>Tree-structured Convolution\n",
    "\n",
    "Graph Convolution\n",
    "- Convolution is shaped by graph structure\n",
    "- E.g. dependency tree is a graph with: Self-loop connection/ Dependency connections/ Reverse connection\n",
    "\n",
    "Summary\n",
    "Neural Sequence Models\n",
    "Sequence of words(characters) --> Composition Function --> Vectors\n",
    "How do we make the choices of different neural sequence models?\n",
    "=> the design philosophy of a model:\n",
    "1. Inductive bias: the set of assumptionns that the learner uses to predict outputs given inputs that it has not encountered.\n",
    "2. Structural bias: a set of prior knowledge incorporated into your model design\n",
    "\n",
    "Structurl bias\n",
    "1. Locality: Local/ Non-local\n",
    "2. Topological structure: Sequential/ Tree/ Graph\n",
    "\n",
    "What inductive bias does a neural component entail?\n",
    "1. local+Sequential: RNN, CNN\n",
    "2. Non-local+Tree: Structured CNN\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "week4\n",
    "NLP and Sequential Data\n",
    "\n",
    "Long-distance Dependencies in Language\n",
    "- Agreement in nnumber, gender, etc\n",
    "- Selectional preference\n",
    "\n",
    "Winograd Schema Challenge\n",
    "\n",
    "Recurrent Neural Networks\n",
    "- Tools to remember information\n",
    "- Why not do back-propagation right after each prediction, instead, do back-propagation after all predictions? Because the 1st one require quatratic time complexity to calculate while the 2nd one is linear.\n",
    "- Forward-propagation, backward-propagation and update can be seperated. In SGD, for each backward-propagation, the update can be very unstable. So, we might do multiple times of forward and backward propagation before update the weights.\n",
    "- Before update, the weight matrices(parameters) are shared. Derivatives are accumulated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
